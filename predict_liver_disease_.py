# -*- coding: utf-8 -*-
"""Predict Liver Disease .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KKBL2SMbMtUnKHMYUfXyRfqYdSDIefsh
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
df = pd.read_csv('project-data.csv', delimiter=';')
df.head()

df.shape
df.dtypes
df.columns
df.columns = df.columns.str.strip()

df['protein'].unique()

df['category'].unique()

plt.hist(df['category'])
plt.xlabel('Category')
plt.ylabel('Frequency')
plt.title('Distribution of Categories')
plt.show()

# Convert 'protein' column to numeric, handling errors
df['protein'] = pd.to_numeric(df['protein'], errors='coerce')

# Now check the data types again
df.dtypes

# Handle missing values (Impute with the mean for numerical, mode for categorical)
imputer_num = SimpleImputer(strategy='mean')
df.iloc[:, 3:-1] = imputer_num.fit_transform(df.iloc[:, 3:-1])

# Replace null values in 'protein' column with the mean
df['protein'].fillna(df['protein'].mean(), inplace=True)

df.isna().sum()

df

# Create box plots for numerical features
df.boxplot(figsize=(12, 6), grid=False)

# Optionally, set titles and labels
plt.title('Box Plots of Numerical Features to Identify Outliers')
plt.xticks(rotation=90)
plt.show()

import warnings
warnings.filterwarnings('ignore')

# Assuming 'df' is your DataFrame and you want to handle outliers in numerical columns
for column in df.select_dtypes(include=np.number).columns:
    # Calculate the IQR (Interquartile Range)
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1

    # Define bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Replace outliers with the mean, mode, or median
    df[column] = np.where(df[column] < lower_bound, df[column].mean(), df[column])  # Replace with mean
    # Or: df[column] = np.where(df[column] < lower_bound, df[column].median(), df[column])  # Replace with median
    # Or: df[column] = np.where(df[column] < lower_bound, df[column].mode()[0], df[column])  # Replace with mode

    df[column] = np.where(df[column] > upper_bound, df[column].mean(), df[column])  # Replace with mean
    # Or: df[column] = np.where(df[column] > upper_bound, df[column].median(), df[column])  # Replace with median
    # Or: df[column] = np.where(df[column] > upper_bound, df[column].mode()[0], df[column])  # Replace with mode

# Encoding categorical variable 'sex'
encoder = LabelEncoder()
df['category'] = encoder.fit_transform(df['category'])
df['sex'] = encoder.fit_transform(df['sex'])  # f -> 0, m -> 1

df['category']

# Split data into features (X) and target (y)
X = df.drop('category', axis=1)
y = df['category']

# Train-test split (80-20 split)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

df

plt.figure(figsize=(10, 6))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

df.shape

df['category'].unique()

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from sklearn.ensemble import RandomForestClassifier

# Initialize models
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(),
    'Naive Bayes': GaussianNB(),
    'XGBoost': XGBClassifier(),
    'Random Forest': RandomForestClassifier()
}

# Train and evaluate models
model_results = {}

for name, model in models.items():
    # The fit method should only take training data
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    class_report = classification_report(y_test, y_pred)
    model_results[name] = {'accuracy': accuracy, 'report': class_report}

# Display model performance
for name, result in model_results.items():
    print(f"Model: {name}")
    print(f"Accuracy: {result['accuracy']}")
    print(f"Classification Report:\n{result['report']}")
    print("-" * 50)

# Create a dictionary to map encoded category values to their original names
category_mapping = {
    3: 'no disease',
    4: 'suspect_disease',
    2: 'hepatitis',
    1: 'fibrosis',
    0: 'cirrhosis'
    # Add more mappings if needed
}

import streamlit as st


# Streamlit app
st.title('Model Deployment: XGBClassifier')

st.sidebar.header('User Input Parameters')

def user_input_features():
    # Get the feature names from your training data
    feature_names = X_train.columns

    # Create input widgets for each feature, ensuring they match the
    # order and names in 'feature_names'
    input_data = {}
    for feature in feature_names:
        if feature == 'sex':
            input_data[feature] = st.sidebar.selectbox(feature.capitalize(), ('0', '1'))
        elif feature in ['age', 'albumin', 'alkaline_phosphatase', 'alanine_aminotransferase',
                       'aspartate_aminotransferase', 'bilirubin', 'cholinesterase',
                       'cholesterol', 'creatinina', 'gamma_glutamyl_transferase', 'protein']:
            input_data[feature] = st.sidebar.number_input(feature.capitalize())
        # Add more conditions if you have other types of features

    features = pd.DataFrame(input_data, index=[0])

    # Convert 'sex' column to numeric (int)
    features['sex'] = pd.to_numeric(features['sex'])

    return features

df_input = user_input_features()
st.subheader('User Input parameters')
st.write(df_input)

# Assuming your Logistic Regression model is named 'clf'
clf = XGBClassifier()  # Initialize or load your trained model
clf.fit(X_train, y_train)  # Fit the model using your original training data

prediction = clf.predict(df_input)
prediction_proba = clf.predict_proba(df_input)

st.subheader('Predicted Result')
# Use the category_mapping to get the original category name
predicted_category_name = category_mapping.get(prediction[0], 'Unknown')
st.write(predicted_category_name)

st.subheader('Prediction Probability')
st.write(prediction_proba)

from sklearn.pipeline import Pipeline
from sklearn.pipeline import FeatureUnion
from sklearn.preprocessing import StandardScaler, OneHotEncoder


# create feature union
features = []
features.append(XGBClassifier())
feature_union = FeatureUnion(features)

# create pipeline
estimators = []
estimators.append(('feature_union', feature_union))
estimators.append(('logistic', LogisticRegression(max_iter=300)))
model = Pipeline(estimators)

from pandas import read_csv
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# load data
dataframe = read_csv('project-data.csv', delimiter=';')
array = dataframe.values
X = array[:,1:]
Y = array[:,0]

# create pipeline
estimators = []
estimators.append(('standardize', StandardScaler()))
estimators.append(('log', LogisticRegression()))
model = Pipeline(estimators)

df

